{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bb6a9ea",
   "metadata": {},
   "source": [
    "<h1><center> <b>Google Summer of Code 2021 </b></center></h1>\n",
    "\n",
    "![alt text](images/gsoc.png)\n",
    "\n",
    "**Organization:** Machine Learning for Science (ML4SCI) Umbrella Organization <br/>\n",
    "**Project Title:** Background Estimation with Neural Autoregressive Flows <br/>\n",
    "**Code author:** Sinan Gençoğlu | sinan.gencogluu@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f04f7",
   "metadata": {},
   "source": [
    "### Project Definition\n",
    "\n",
    "* Data-driven background estimation is crucial for many scientific searches, including searches for new phenomena in experimental datasets. Neural autoregressive flows (NAF) is a deep generative model that can be used for general transformations and is therefore attractive for this application. The MLBENDER project focuses on studying how to develop such transformations that can be learned and applied to a region of interest. In this project the main aim is implementing a Neural Autoregressive Flow (NAF) model to estimate the background distribution and apply it to a representative physics analysis searching for a resonance excess over a smooth background.\n",
    "\n",
    "##### Terminology: Normalizing Flows \n",
    "\n",
    "* In order to estimate the probability of some distributions(in this case: background distributions of events), we can use some generative models that are capable of estimating the probabilities. Normalizing flow models are powerful distributions approximaters. Basically,  a normalizing flow transforms a simple distribution into a complex one by applying a sequence of invertible transformation functions.\n",
    "\n",
    "![alt-text](images/nf.png)\n",
    "\n",
    "<center>Reference: <a href=\"Reference from:\">https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html </a></center><br>\n",
    "\n",
    "* In our case I chose to implement Masked Autoregressive Flow [MAF  Papamakarios et al., 2017](https://arxiv.org/abs/1705.07057). In addition to that, I also had another normalizing flow model, RealNVP, in case I couldn't get enough performance from MAF. I do not go into detail about RealNVP because it is out of scope for this part of the study.\n",
    "\n",
    "* MAF tries to generate samples conditioned on previous ones. This is a sequential process. You can see representation of the generation below. \n",
    "\n",
    "\n",
    "<center><b>Conditional Generation Process of the Masked Autoregressive Flow</b></center><br>\n",
    "\n",
    "![alt-text](images/maf.png)\n",
    "<center>Reference: <a href=\"Reference from:\">https://blog.evjang.com/2018/01/nf2.html </a></center><br>\n",
    "\n",
    "* The gray unit $x_i$ is the unit we are trying to compute, and the blue units are the values it depends on. $α_i$ and $μ_i$ are scalars that are computed by passing $x_{1:i−1}$ through neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610df94",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "* I have used a couple of datasets during the studies. For example, first dataset is [Large Hadron Collider (LHC) Dataset](https://zenodo.org/record/2629073). It consists of ~1M jet events and events contains some particles. For each event, all particles in the event are assumed to be massless and are recorded in detector coordinates (pT, eta, phi). These are represented as a DataFrame and each event consist of 21K+1 features (last feature defines that whether the event is coming from the background or signal part of the data, other features is presented in groups of 3 consecutive features. Details can be found on the previous link). Other dataset is relatively same as before but this time we have 76+1 features to estimate.\n",
    "\n",
    "<center> <b>Sample of the dataset</b></center>\n",
    "\n",
    "![alt-text](images/data2_1.png)\n",
    "\n",
    "![alt-text](images/data_1.jpeg)\n",
    "\n",
    "* All features represent a jet. So, we have 76 jets and 1 feature that defines whether the jets are coming from background or signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3063438d",
   "metadata": {},
   "source": [
    "### Pre-processing steps\n",
    "\n",
    "* First we import the neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127ddf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS # \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import MAF as fnn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4495587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the data.\n",
    "df = pd.read_parquet(\"TTTT_DNN_nJ4_nB2_2018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f9871c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804642, 77)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to use background based events you can change type: 0 for background, 1 for signal\n",
    "data_type = 0 # background\n",
    "data = df.loc[df[df.columns.values[-1]]==data_type]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "293304cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AK4HTpMETpLepPt</th>\n",
       "      <th>minMleppBjet</th>\n",
       "      <th>mass_minBBdr</th>\n",
       "      <th>deltaR_lepBJet_maxpt</th>\n",
       "      <th>lepDR_minBBdr</th>\n",
       "      <th>centrality</th>\n",
       "      <th>deltaEta_maxBB</th>\n",
       "      <th>aveCSVpt</th>\n",
       "      <th>aveBBdr</th>\n",
       "      <th>FW_momentum_0</th>\n",
       "      <th>...</th>\n",
       "      <th>HOTGoodTrijet1_dRtridijet</th>\n",
       "      <th>HOTGoodTrijet1_csvJetnotdijet</th>\n",
       "      <th>HOTGoodTrijet1_dRtrijetJetnotdijet</th>\n",
       "      <th>HOTGoodTrijet2_mass</th>\n",
       "      <th>HOTGoodTrijet2_dijetmass</th>\n",
       "      <th>HOTGoodTrijet2_pTratio</th>\n",
       "      <th>HOTGoodTrijet2_dRtridijet</th>\n",
       "      <th>HOTGoodTrijet2_csvJetnotdijet</th>\n",
       "      <th>HOTGoodTrijet2_dRtrijetJetnotdijet</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231977</th>\n",
       "      <td>682.360291</td>\n",
       "      <td>76.110710</td>\n",
       "      <td>51.740883</td>\n",
       "      <td>1.891567</td>\n",
       "      <td>1.964009</td>\n",
       "      <td>0.460657</td>\n",
       "      <td>-2.927261</td>\n",
       "      <td>0.626476</td>\n",
       "      <td>2.518530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231978</th>\n",
       "      <td>913.399109</td>\n",
       "      <td>102.672829</td>\n",
       "      <td>854.941467</td>\n",
       "      <td>4.175213</td>\n",
       "      <td>3.589615</td>\n",
       "      <td>0.515980</td>\n",
       "      <td>3.121105</td>\n",
       "      <td>0.694751</td>\n",
       "      <td>3.202137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231979</th>\n",
       "      <td>788.331055</td>\n",
       "      <td>68.620819</td>\n",
       "      <td>82.993179</td>\n",
       "      <td>0.494603</td>\n",
       "      <td>2.828682</td>\n",
       "      <td>0.366740</td>\n",
       "      <td>-1.837404</td>\n",
       "      <td>0.986366</td>\n",
       "      <td>2.461292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126494</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.918655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231980</th>\n",
       "      <td>757.287476</td>\n",
       "      <td>120.900139</td>\n",
       "      <td>220.880264</td>\n",
       "      <td>1.906771</td>\n",
       "      <td>2.003259</td>\n",
       "      <td>0.587130</td>\n",
       "      <td>1.707826</td>\n",
       "      <td>0.847611</td>\n",
       "      <td>2.374820</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231981</th>\n",
       "      <td>677.532288</td>\n",
       "      <td>39.759781</td>\n",
       "      <td>131.753311</td>\n",
       "      <td>0.678705</td>\n",
       "      <td>3.126003</td>\n",
       "      <td>0.777935</td>\n",
       "      <td>-1.066363</td>\n",
       "      <td>1.004760</td>\n",
       "      <td>3.194130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123956</td>\n",
       "      <td>0.045146</td>\n",
       "      <td>0.893217</td>\n",
       "      <td>184.401566</td>\n",
       "      <td>92.719444</td>\n",
       "      <td>0.83411</td>\n",
       "      <td>0.317127</td>\n",
       "      <td>0.998275</td>\n",
       "      <td>1.508981</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AK4HTpMETpLepPt  minMleppBjet  mass_minBBdr  deltaR_lepBJet_maxpt  \\\n",
       "231977       682.360291     76.110710     51.740883              1.891567   \n",
       "231978       913.399109    102.672829    854.941467              4.175213   \n",
       "231979       788.331055     68.620819     82.993179              0.494603   \n",
       "231980       757.287476    120.900139    220.880264              1.906771   \n",
       "231981       677.532288     39.759781    131.753311              0.678705   \n",
       "\n",
       "        lepDR_minBBdr  centrality  deltaEta_maxBB  aveCSVpt   aveBBdr  \\\n",
       "231977       1.964009    0.460657       -2.927261  0.626476  2.518530   \n",
       "231978       3.589615    0.515980        3.121105  0.694751  3.202137   \n",
       "231979       2.828682    0.366740       -1.837404  0.986366  2.461292   \n",
       "231980       2.003259    0.587130        1.707826  0.847611  2.374820   \n",
       "231981       3.126003    0.777935       -1.066363  1.004760  3.194130   \n",
       "\n",
       "        FW_momentum_0  ...  HOTGoodTrijet1_dRtridijet  \\\n",
       "231977            1.0  ...                   0.000000   \n",
       "231978            1.0  ...                   0.000000   \n",
       "231979            1.0  ...                   0.126494   \n",
       "231980            1.0  ...                   0.000000   \n",
       "231981            1.0  ...                   0.123956   \n",
       "\n",
       "        HOTGoodTrijet1_csvJetnotdijet  HOTGoodTrijet1_dRtrijetJetnotdijet  \\\n",
       "231977                       0.000000                            0.000000   \n",
       "231978                       0.000000                            0.000000   \n",
       "231979                       0.020007                            0.918655   \n",
       "231980                       0.000000                            0.000000   \n",
       "231981                       0.045146                            0.893217   \n",
       "\n",
       "        HOTGoodTrijet2_mass  HOTGoodTrijet2_dijetmass  HOTGoodTrijet2_pTratio  \\\n",
       "231977             0.000000                  0.000000                 0.00000   \n",
       "231978             0.000000                  0.000000                 0.00000   \n",
       "231979             0.000000                  0.000000                 0.00000   \n",
       "231980             0.000000                  0.000000                 0.00000   \n",
       "231981           184.401566                 92.719444                 0.83411   \n",
       "\n",
       "        HOTGoodTrijet2_dRtridijet  HOTGoodTrijet2_csvJetnotdijet  \\\n",
       "231977                   0.000000                       0.000000   \n",
       "231978                   0.000000                       0.000000   \n",
       "231979                   0.000000                       0.000000   \n",
       "231980                   0.000000                       0.000000   \n",
       "231981                   0.317127                       0.998275   \n",
       "\n",
       "        HOTGoodTrijet2_dRtrijetJetnotdijet  type  \n",
       "231977                            0.000000   0.0  \n",
       "231978                            0.000000   0.0  \n",
       "231979                            0.000000   0.0  \n",
       "231980                            0.000000   0.0  \n",
       "231981                            1.508981   0.0  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ceefe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AK4HTpMETpLepPt\n",
      "1 minMleppBjet\n",
      "2 mass_minBBdr\n",
      "3 deltaR_lepBJet_maxpt\n",
      "4 lepDR_minBBdr\n",
      "5 centrality\n",
      "6 deltaEta_maxBB\n",
      "7 aveCSVpt\n",
      "8 aveBBdr\n",
      "9 FW_momentum_0\n",
      "10 FW_momentum_1\n",
      "11 FW_momentum_2\n",
      "12 FW_momentum_3\n",
      "13 FW_momentum_4\n",
      "14 FW_momentum_5\n",
      "15 FW_momentum_6\n",
      "16 mass_maxJJJpt\n",
      "17 BJetLeadPt\n",
      "18 deltaR_minBB\n",
      "19 minDR_lepBJet\n",
      "20 MT_lepMet\n",
      "21 AK4HT\n",
      "22 hemiout\n",
      "23 theJetLeadPt\n",
      "24 corr_met_MultiLepCalc\n",
      "25 leptonPt_MultiLepCalc\n",
      "26 mass_lepJets0\n",
      "27 mass_lepJets1\n",
      "28 mass_lepJets2\n",
      "29 MT2bb\n",
      "30 mass_lepBJet0\n",
      "31 mass_lepBJet_mindr\n",
      "32 secondJetPt\n",
      "33 fifthJetPt\n",
      "34 sixthJetPt\n",
      "35 PtFifthJet\n",
      "36 mass_minLLdr\n",
      "37 mass_maxBBmass\n",
      "38 deltaR_lepJetInMinMljet\n",
      "39 deltaPhi_lepJetInMinMljet\n",
      "40 deltaR_lepbJetInMinMlb\n",
      "41 deltaPhi_lepbJetInMinMlb\n",
      "42 M_allJet_W\n",
      "43 HT_bjets\n",
      "44 ratio_HTdHT4leadjets\n",
      "45 csvJet3\n",
      "46 csvJet4\n",
      "47 firstcsvb_bb\n",
      "48 secondcsvb_bb\n",
      "49 thirdcsvb_bb\n",
      "50 fourthcsvb_bb\n",
      "51 NJets_JetSubCalc\n",
      "52 HT_2m\n",
      "53 Sphericity\n",
      "54 Aplanarity\n",
      "55 minDR_lepJet\n",
      "56 BDTtrijet1\n",
      "57 BDTtrijet2\n",
      "58 BDTtrijet3\n",
      "59 BDTtrijet4\n",
      "60 NresolvedTops1pFake\n",
      "61 NJetsTtagged\n",
      "62 NJetsWtagged\n",
      "63 NJetsCSV_MultiLepCalc\n",
      "64 HOTGoodTrijet1_mass\n",
      "65 HOTGoodTrijet1_dijetmass\n",
      "66 HOTGoodTrijet1_pTratio\n",
      "67 HOTGoodTrijet1_dRtridijet\n",
      "68 HOTGoodTrijet1_csvJetnotdijet\n",
      "69 HOTGoodTrijet1_dRtrijetJetnotdijet\n",
      "70 HOTGoodTrijet2_mass\n",
      "71 HOTGoodTrijet2_dijetmass\n",
      "72 HOTGoodTrijet2_pTratio\n",
      "73 HOTGoodTrijet2_dRtridijet\n",
      "74 HOTGoodTrijet2_csvJetnotdijet\n",
      "75 HOTGoodTrijet2_dRtrijetJetnotdijet\n",
      "76 type\n"
     ]
    }
   ],
   "source": [
    "# We can take a look at the all features(jets)\n",
    "for i, col in enumerate(data.columns):\n",
    "    print(i, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ac054",
   "metadata": {},
   "source": [
    "### Normalizing the data\n",
    "\n",
    "* Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. \n",
    "\n",
    "* In our case, our dataset have so many features that have different range of values. We need to normalize these values in order to get good results. Since there are too many variables with variable values in the dataset, it is very difficult to generate the background distributions of all the features one by one by applying a single normalization method. Therefore, for a more efficient training, I divided the data into 3 with similar value range features together. First part contains attributes with continuous values, second part contains features with discrete values, and the last part contains the features with sparse values. I will show the one part of the results, you can generate samples for the other parts easily.\n",
    "\n",
    "* Before I got my final results, I have used couple of normalizations but I will show the one that works reasonably well in this case which is MinMaxScaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9007fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dataloader for training the model.\n",
    "\n",
    "class TTTT_Data(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.len = len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.FloatTensor(self.data[index])\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2405f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(data.values)\n",
    "train_data = TTTT_Data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab6cb1",
   "metadata": {},
   "source": [
    "* It is crucial to keep our scaler out of the TTTT_Data class, we will need it in the further processes. Since our dataset has different range of values. For example, if it contains images we can simply divide it by 255 then denormalize it by multiplying with 255. But in this case it is not possible and we have to use inverse of the function that we used in the normalization part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8992294",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd6f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 8 # Feature size\n",
    "num_hidden = 64 # Number of hidden layers\n",
    "num_cond_inputs = False\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "block_size = 7\n",
    "\n",
    "# Optimizers\n",
    "gamma = 0.1\n",
    "step_size = 5 # After 5 epoch decrease learning rate by gamma.\n",
    "\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6190aca",
   "metadata": {},
   "source": [
    "### Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = TTTT_Data(train_data)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "modules = []\n",
    "\n",
    "for _ in range(block_size):\n",
    "    modules += [\n",
    "        fnn.MADE(num_inputs, num_hidden, num_cond_inputs, act=act),\n",
    "        fnn.BatchNormFlow(num_inputs),\n",
    "        fnn.Reverse(num_inputs)\n",
    "    ]\n",
    "\n",
    "model = fnn.FlowSequential(*modules)\n",
    "model.train()\n",
    "\n",
    "for module in model.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.orthogonal_(module.weight)\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            module.bias.data.fill_(0)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "loss_training = []\n",
    "\n",
    "print(\"Model is Cuda?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "def train(epoch, loss_training):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = -model.log_probs(data, None).mean()\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1)%2000==0:\n",
    "          print(\"Loss:\", train_loss/(batch_idx+1))\n",
    "          print(\"Iteration:\", batch_idx+1)\n",
    "    \n",
    "    loss_training.append(train_loss/len(train_loader.dataset))\n",
    "    scheduler.step()\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('\\nEpoch: {}'.format(epoch+1))\n",
    "    train(epoch, loss_training)\n",
    "torch.save(model.state_dict(), 'model' + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f70519",
   "metadata": {},
   "source": [
    "* Training process can take some time. These are the lost curve using only small part of the data.\n",
    "\n",
    "<center><b>Loss vs Epoch Curve</b></center>\n",
    "\n",
    "![](images/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b1fff",
   "metadata": {},
   "source": [
    "### Generating Samples \n",
    "\n",
    "* To generate samples from the model you can switch to evaluation mode by using **model.eval()** then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bbfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5000\n",
    "\n",
    "# Generate random samples\n",
    "fixed_noise = torch.Tensor(num_samples, num_inputs).normal_()\n",
    "\n",
    "# Get model predictions\n",
    "samples = model.sample(num_samples, noise=fixed_noise).detach().cpu().numpy()\n",
    "\n",
    "# Denormalize samples\n",
    "normalized = scaler.inverse_transform(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13fa8b",
   "metadata": {},
   "source": [
    "* After generating samples from the model you can compare how your model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc757c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 14))\n",
    "rows = 4\n",
    "cols = 2\n",
    "axes=[]\n",
    "for i in range(rows*cols):\n",
    "    axes.append(fig.add_subplot(rows, cols, i+1))\n",
    "    label = str(data.columns[i])[1:]\n",
    "    plt.xlabel(label)\n",
    "    sns.distplot(data.values[:, i])\n",
    "    sns.distplot(normalized[:, i])\n",
    "\n",
    "fig.tight_layout() \n",
    "fig.legend(labels=['Real_data','Random_generated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85063fe1",
   "metadata": {},
   "source": [
    "![alt-text](images/dists.png)\n",
    "\n",
    "![alt-text](images/results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46419598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinandl",
   "language": "python",
   "name": "sinandl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
